{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df33330-00ed-48cc-a0e7-6e6b69fd1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.uniform import Uniform # to initialise the weights in the NN\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "241e097f-28a8-44cb-a332-f42d3ddc88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lets assume we have 2 simple sentences - \n",
    "Troll2 is great. \n",
    "Gymkata is great.\n",
    "\n",
    "so, our vocab is 4 words/tokens long.\n",
    "When we input Troll2 in the NN, our input would look like 1 0 0 0.\n",
    "1 for Troll2, and 0 for is, great and Gymkata.\n",
    "so, we are basically doing One-Hot-Encoding over our input.\n",
    "\"\"\"\n",
    "\n",
    "inputs = torch.tensor([[1.0, 0., 0., 0.],\n",
    "[0., 1.0, 0., 0.],\n",
    "[0., 0., 1.0, 0.],\n",
    "[0., 0., 0., 1.0]])\n",
    "\n",
    "#the goal is to create a simple NN which can predict the next token.\n",
    "# we are assuming that great predicts Gymkata in our example.\n",
    "\n",
    "labels = torch.tensor([[0., 1.0, 0., 0.], \n",
    "[0., 0., 1.0, 0.],\n",
    "[0., 0., 0., 1.0],\n",
    "[0., 1.0, 0., 0.]])\n",
    "\n",
    "# we combine the input and labels into a tensor dataset\n",
    "dataset = TensorDataset(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633f2cc4-afa9-4006-8191-eefaffb0542f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndataloaders are useful when we have a lot of training data\\nmaking it easy to access data in batches\\neasier to shuffle the data in each epoch\\nmakes it easy to use a small fraction of data\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset) \n",
    "\"\"\"\n",
    "dataloaders are useful when we have a lot of training data\n",
    "making it easy to access data in batches\n",
    "easier to shuffle the data in each epoch\n",
    "makes it easy to use a small fraction of data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea3c66-0f19-4762-91a5-0977800eaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingFromScratch(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # using uniform distribution to generate random weights between 0.5 and -0.5, ie each value has equal probability\n",
    "        min_value = -0.5\n",
    "        max_value = 0.5\n",
    "\n",
    "        self.input1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.output1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output5_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.loss= nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = inputs[0] # since we have a list of lists\n",
    "        inputs_to_top_hidden = (input[0] * self.input1_w1) + (input[1] * self.input2_w1) + (input[2] * self.input3_w1) + (input[3] * self.input4_w1)\n",
    "        inputs_to_bottom_hidden = (input[0] * self.input1_w2) + (input[1] * self.input2_w2) + (input[2] * self.input3_w2) + (input[3] * self.input4_w2)\n",
    "        # since the activation functions are the identity functions ie input is the same as the output \n",
    "\n",
    "        output1 = inputs_to_top_hidden * self.output1_w1 + inputs_to_bottom_hidden * self.output1_w2\n",
    "        output2 = inputs_to_top_hidden * self.output2_w1 + inputs_to_bottom_hidden * self.output2_w2\n",
    "        output3 = inputs_to_top_hidden * self.output3_w1 + inputs_to_bottom_hidden * self.output3_w2\n",
    "        output4 = inputs_to_top_hidden * self.output4_w1 + inputs_to_bottom_hidden * self.output4_w2\n",
    "\n",
    "        # the cross entropy loss function that we are using for backpropagation does the softmax for us\n",
    "        # packaging up the output values using torch.stack\n",
    "        # if instead of using torch.stack, we just returned a list of output variables by wrapping them in square brackets, then the gradients would get stripped off\n",
    "        # and we wont be able to do backpropagation\n",
    "        output_presoftmax = torch.stack ([output1, output2, output3, output4]) \n",
    "        return output_presoftmax\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr = 0.1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # split the batch of training data into input and labels\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i)\n",
    "        loss = self.loss(output_i, label_i[0]) # cross entropy itself runs the output values through the softmax activation function\n",
    "                                               # and quantifies the difference between the predicted softmax values and the actual target values\n",
    "        return loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
