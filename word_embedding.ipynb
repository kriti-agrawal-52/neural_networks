{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df33330-00ed-48cc-a0e7-6e6b69fd1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.uniform import Uniform # to initialise the weights in the NN\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e097f-28a8-44cb-a332-f42d3ddc88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lets assume we have 2 simple sentences - \n",
    "Troll2 is great. \n",
    "Gymkata is great.\n",
    "\n",
    "so, our vocab is 4 words/tokens long.\n",
    "When we input Troll2 in the NN, our input would look like 1 0 0 0.\n",
    "1 for Troll2, and 0 for is, great and Gymkata.\n",
    "so, we are basically doing One-Hot-Encoding over our input.\n",
    "\"\"\"\n",
    "\n",
    "inputs = torch.tensor([[1.0, 0., 0., 0.],\n",
    "[0., 1.0, 0., 0.],\n",
    "[0., 0., 1.0, 0.],\n",
    "[0., 0., 0., 1.0]])\n",
    "\n",
    "#the goal is to create a simple NN which can predict the next token.\n",
    "# we are assuming that great predicts Gymkata in our example.\n",
    "\n",
    "labels = torch.tensor([[0., 1.0, 0., 0.], \n",
    "[0., 0., 1.0, 0.],\n",
    "[0., 0., 0., 1.0],\n",
    "[0., 1.0, 0., 0.]])\n",
    "\n",
    "# we combine the input and labels into a tensor dataset\n",
    "dataset = TensorDataset(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f2cc4-afa9-4006-8191-eefaffb0542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset) \n",
    "\"\"\"\n",
    "dataloaders are useful when we have a lot of training data\n",
    "making it easy to access data in batches\n",
    "easier to shuffle the data in each epoch\n",
    "makes it easy to use a small fraction of data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea3c66-0f19-4762-91a5-0977800eaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingFromScratch(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # using uniform distribution to generate random weights between 0.5 and -0.5, ie each value has equal probability\n",
    "        min_value = -0.5\n",
    "        max_value = 0.5\n",
    "\n",
    "        self.input1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.output1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.loss= nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = input[0] # since we have a list of lists\n",
    "        inputs_to_top_hidden = (input[0] * self.input1_w1) + (input[1] * self.input2_w1) + (input[2] * self.input3_w1) + (input[3] * self.input4_w1)\n",
    "        inputs_to_bottom_hidden = (input[0] * self.input1_w2) + (input[1] * self.input2_w2) + (input[2] * self.input3_w2) + (input[3] * self.input4_w2)\n",
    "        # since the activation functions are the identity functions ie input is the same as the output \n",
    "\n",
    "        output1 = inputs_to_top_hidden * self.output1_w1 + inputs_to_bottom_hidden * self.output1_w2\n",
    "        output2 = inputs_to_top_hidden * self.output2_w1 + inputs_to_bottom_hidden * self.output2_w2\n",
    "        output3 = inputs_to_top_hidden * self.output3_w1 + inputs_to_bottom_hidden * self.output3_w2\n",
    "        output4 = inputs_to_top_hidden * self.output4_w1 + inputs_to_bottom_hidden * self.output4_w2\n",
    "\n",
    "        # the cross entropy loss function that we are using for backpropagation does the softmax for us\n",
    "        # packaging up the output values using torch.stack\n",
    "        # if instead of using torch.stack, we just returned a list of output variables by wrapping them in square brackets, then the gradients would get stripped off\n",
    "        # and we wont be able to do backpropagation\n",
    "        output_presoftmax = torch.stack ([output1, output2, output3, output4]) \n",
    "        return output_presoftmax\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr = 0.1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # split the batch of training data into input and labels\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i)\n",
    "        loss = self.loss(output_i, label_i[0]) # cross entropy itself runs the output values through the softmax activation function\n",
    "                                               # and quantifies the difference between the predicted softmax values and the actual target values\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aada94c-e44f-4959-a69b-7749c8632445",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_scratch = WordEmbeddingFromScratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6f628-09f2-4a03-b020-0f0eda58412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before optimization the parameters are: {[ (name, param.data) for name, param in model_from_scratch.named_parameters() ]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8582cd55-62d8-465a-9965-f6338a785f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pandas dataframe with embeddings for each input token\n",
    "param_dict = {}\n",
    "for name, param in model_from_scratch.named_parameters():\n",
    "    if name.startswith(\"input\") and \"_\" in name:\n",
    "        word, weight = name.split('_')\n",
    "        if word not in param_dict:\n",
    "            param_dict[word]={}\n",
    "        param_dict[word][weight] = param.item()\n",
    "\n",
    "# convert to dataframe \n",
    "df = pd.DataFrame.from_dict(param_dict, orient = 'index')\n",
    "df.index.name = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec23eb-220f-45a8-9d36-9980fb12b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token'] = ['Troll2', 'is', 'great','Gymkata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69b32d-151c-49e5-8ab2-16bb59c67a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12353d88-2c20-4ecd-85ff-1ab6ce1aa948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "sns.scatterplot(x=df['w1'], y=df['w2'], s=100, color='green', marker='o')\n",
    "\n",
    "# Add labels for each point\n",
    "for word, row in df.iterrows():\n",
    "    plt.text(row['w1'] + 0.01, row['w2'] + 0.01, word, fontsize=9)\n",
    "\n",
    "# Axis labels and title\n",
    "plt.xlabel(\"Weight 1 (w1)\")\n",
    "plt.ylabel(\"Weight 2 (w2)\")\n",
    "plt.title(\"2D Weight Embeddings for Input Tokens\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b5b3f-adee-4902-a7a9-1b0fe0407c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the NN\n",
    "trainer = L.Trainer(max_epochs = 200)\n",
    "trainer.fit(model_from_scratch, train_dataloaders = dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42087b43-6977-42e8-9928-5cc3136fd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with new word embeddings\n",
    "param_after_training_dict = {}\n",
    "for name, param in model_from_scratch.named_parameters():\n",
    "    if name.startswith('input') and \"_\" in name:\n",
    "        input_num, weight = name.split('_')\n",
    "        if input_num not in param_after_training_dict:\n",
    "            param_after_training_dict[input_num] = {}\n",
    "        param_after_training_dict[input_num][weight] = param.item()\n",
    "\n",
    "# convert to dataframe \n",
    "df_after_training = pd.DataFrame.from_dict(param_after_training_dict, orient = 'index')\n",
    "df_after_training.index.name = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79c0a5-bbc8-4a88-89b0-822ba8fccab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_training['token'] = ['Troll2', 'is', 'great','Gymkata']\n",
    "df_after_training.set_index('token', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23cc244-0542-4cef-97c7-6755f73bff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ad538-fd99-426e-b9db-7473563ff21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "sns.scatterplot(x=df_after_training['w1'], y=df_after_training['w2'], s=100, color='green', marker='o')\n",
    "\n",
    "# Add labels for each point\n",
    "for token, row in df_after_training.iterrows():\n",
    "    plt.text(row['w1'] + 0.01, row['w2'] + 0.01, token, fontsize=9)\n",
    "\n",
    "# Axis labels and title\n",
    "plt.xlabel(\"Weight 1 (w1)\")\n",
    "plt.ylabel(\"Weight 2 (w2)\")\n",
    "plt.title(\"2D Weight Embeddings for Input Tokens\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301229da-2013-4e4e-820d-5b10074a5219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
